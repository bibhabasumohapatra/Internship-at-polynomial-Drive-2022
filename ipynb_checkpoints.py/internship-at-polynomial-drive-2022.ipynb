{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Most difficult thing for me was to start at first, I already had a plan to use LSTM networks and wrote Dataset but everything came to Data at last.  I was given a huge dataset massive 5 GB around thats also in json file, and lost hope to start with notebook,\n##### But evantually now I had a solution, to not only convert but also compress.  I went to kaggle created a notebook uploaded the json(took time) then ran import json import csv\n##### and in the same file droped unnecessary columns, clicked save run and now I am using the csv file only of 2 GB\nhttps://www.kaggle.com/bibhabasumohapatra/amazon-dataset-csv-generator/data?select=PolynomialInternshipDrive2022.csv\nnow, https://www.youtube.com/watch?v=oreIJQZ40H0&t=636s","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import torch\nimport transformers\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:22:31.318558Z","iopub.execute_input":"2022-02-17T04:22:31.319096Z","iopub.status.idle":"2022-02-17T04:22:32.841691Z","shell.execute_reply.started":"2022-02-17T04:22:31.318998Z","shell.execute_reply":"2022-02-17T04:22:32.840971Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 512\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 10\nTOKENIZER = transformers.BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True, max_len = MAX_LEN , padding = 'max_length')","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:22:32.843219Z","iopub.execute_input":"2022-02-17T04:22:32.843483Z","iopub.status.idle":"2022-02-17T04:22:37.753534Z","shell.execute_reply.started":"2022-02-17T04:22:32.843447Z","shell.execute_reply":"2022-02-17T04:22:37.752863Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a17ead5b98549c68c9658c96d8c5b36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b64eacface0d4f07aadaa49b66202a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92921d8221af425996061b1fa2dcfdb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf49b2fefe194dbebdcc2948ce6e5931"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Dataset.py","metadata":{}},{"cell_type":"code","source":"class AmazonDataset(torch.utils.data.Dataset):\n    def __init__(self, review, target):\n        #\n\n        self.review = review\n        self.target = target\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        \n    def __len__(self):\n        return len(self.review)\n    \n    def __getitem__(self, item):\n        \n        review = str(self.review[item])\n        review = \" \".join(review.split())\n        \n        inputs = self.tokenizer.encode_plus(\n        review,\n        None,\n        add_special_tokens=True,\n        max_length=self.max_len,\n        pad_to_max_length=True,)\n        \n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n        return {\n        \"ids\": torch.tensor(\n        ids, dtype=torch.long\n        ),\n        \"mask\": torch.tensor(\n        mask, dtype=torch.long\n        ),\n        \"token_type_ids\": torch.tensor(\n        token_type_ids, dtype=torch.long\n        ),\n        \"targets\": torch.tensor(\n        self.target[item], dtype=torch.float\n        )\n        }","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:22:39.133637Z","iopub.execute_input":"2022-02-17T04:22:39.134529Z","iopub.status.idle":"2022-02-17T04:22:39.145679Z","shell.execute_reply.started":"2022-02-17T04:22:39.134482Z","shell.execute_reply":"2022-02-17T04:22:39.145048Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"## LOSS FUNCTION\n\ndef loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:22:39.331968Z","iopub.execute_input":"2022-02-17T04:22:39.332268Z","iopub.status.idle":"2022-02-17T04:22:39.338316Z","shell.execute_reply.started":"2022-02-17T04:22:39.332227Z","shell.execute_reply":"2022-02-17T04:22:39.337604Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Engine.py","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(data_loader, model, optimizer, device, scheduler):\n    \n    model.train()\n    for d in data_loader:\n \n \n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets = d[\"targets\"]\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(\n        ids=ids,\n        mask=mask,\n        token_type_ids=token_type_ids\n        )\n\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    \n    \ndef evaluate(data_loader, model, device):\n \n    model.eval()\n    \n    fin_targets = []\n    fin_outputs = []\n\n    with torch.no_grad():\n    \n        for d in data_loader:\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n            outputs = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n            )\n            targets = targets.cpu().detach()\n            fin_targets.extend(targets.numpy().tolist())\n            \n        outputs = torch.sigmoid(outputs).cpu().detach()\n        fin_outputs.extend(outputs.numpy().tolist())\n    return fin_outputs, fin_targets","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:22:40.928631Z","iopub.execute_input":"2022-02-17T04:22:40.928890Z","iopub.status.idle":"2022-02-17T04:22:40.940044Z","shell.execute_reply.started":"2022-02-17T04:22:40.928859Z","shell.execute_reply":"2022-02-17T04:22:40.939391Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Model.py","metadata":{}},{"cell_type":"code","source":"class BERTModel(nn.Module):\n    def __init__(self):\n        super(BERTModel, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 1)\n        \n    def forward(self, ids, mask, token_type_ids):\n        \n        _, x = self.bert(\n        ids,\n        attention_mask=mask,\n        token_type_ids=token_type_ids\n        )\n        x = self.bert_drop(x)\n        x = self.out(x)            \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:22:41.876676Z","iopub.execute_input":"2022-02-17T04:22:41.877225Z","iopub.status.idle":"2022-02-17T04:22:41.883157Z","shell.execute_reply.started":"2022-02-17T04:22:41.877188Z","shell.execute_reply":"2022-02-17T04:22:41.882318Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Main.py","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:22:43.286439Z","iopub.execute_input":"2022-02-17T04:22:43.287323Z","iopub.status.idle":"2022-02-17T04:22:48.057397Z","shell.execute_reply.started":"2022-02-17T04:22:43.287273Z","shell.execute_reply":"2022-02-17T04:22:48.056660Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/amazon-dataset-csv-generator/PolynomialInternshipDrive2022.csv')\n#######################################################\ndf_train, df_valid = train_test_split(df, test_size=0.3, random_state=42, stratify=df.overall.values)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\n\ntrain_dataset = AmazonDataset(review=df_train.review.values, target=df_train.overall.values)\ntrain_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=0,)# collate_fn=collate_fn)\n\nvalid_dataset = AmazonDataset(review=df_valid.review.values,target=df_valid.overall.values)\nvalid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=0,)# collate_fn=collate_fn)\n\ndevice = torch.device(\"cuda\")\nmodel = BERTModel()\nmodel.to('cuda')\n\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {\n \"params\": [\n p for n, p in param_optimizer if\n not any(nd in n for nd in no_decay)\n ],\n \"weight_decay\": 0.001,\n },\n {\n \"params\": [\n p for n, p in param_optimizer if\n any(nd in n for nd in no_decay)\n ],\n \"weight_decay\": 0.0,\n },\n ]\n\nnum_train_steps = int(\n len(df_train) / TRAIN_BATCH_SIZE * EPOCHS\n )\n\noptimizer = AdamW(optimizer_parameters, lr=3e-5)\n\nscheduler = get_linear_schedule_with_warmup(\n optimizer,\n num_warmup_steps=0,\n num_training_steps=num_train_steps\n )\n\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n    train(\n    train_data_loader, model, optimizer, device, scheduler\n    )\noutputs, targets = eval_fn(\n    valid_data_loader, model, device\n )\n\noutputs = np.array(outputs) >= 0.5\naccuracy = metrics.accuracy_score(targets, outputs)\nprint(f\"Accuracy Score = {accuracy}\")\nif accuracy > best_accuracy:\n    torch.save(model.state_dict(), MODEL_PATH)\n    best_accuracy = accuracy","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:22:48.058948Z","iopub.execute_input":"2022-02-17T04:22:48.059196Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbfa931dec594777b15ee711734910ab"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:10:28.081029Z","iopub.status.idle":"2022-02-17T04:10:28.082566Z","shell.execute_reply.started":"2022-02-17T04:10:28.082307Z","shell.execute_reply":"2022-02-17T04:10:28.082335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}